{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('data/cylinder_nektar_wake.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_star = data['U_star'] # N x 2 x T\n",
    "p_star = data['p_star'] # N x 2 x T\n",
    "t_star = data ['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2 \n",
    "\n",
    "N = X_star.shape[0] # 5000\n",
    "T = t_star.shape[0] # 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X_star[:, 0:1]\n",
    "y_test = X_star[:, 1:2]\n",
    "p_test = p_star[:, 0:1]\n",
    "u_test = U_star[:, 0:1, 0]\n",
    "t_test = np.ones((x_test.shape[0], x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n",
    "TT = np.tile(t_star, (1, N)).T  # N x T\n",
    "\n",
    "UU = U_star[:, 0, :]  # N x T\n",
    "VV = U_star[:, 1, :]  # N x T\n",
    "PP = p_star  # N x T\n",
    "\n",
    "x = XX.flatten()[:, None]  # NT x 1\n",
    "y = YY.flatten()[:, None]  # NT x 1\n",
    "t = TT.flatten()[:, None]  # NT x 1\n",
    "\n",
    "u = UU.flatten()[:, None]  # NT x 1\n",
    "v = VV.flatten()[:, None]  # NT x 1\n",
    "p = PP.flatten()[:, None]  # NT x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Train and Test data and add noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10cf43ef0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noiseless data\n",
    "N_train = 5000\n",
    "idx = np.random.choice(N*T, N_train, replace=False)\n",
    "x_train = x[idx,:] # 5000 x 1\n",
    "y_train = y[idx,:]\n",
    "t_train = t[idx,:]\n",
    "u_train = u[idx,:]\n",
    "v_train = v[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy data\n",
    "noise = 0.01        \n",
    "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "v_train = v_train + noise*np.std(v_train)*np.random.randn(v_train.shape[0], v_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "\n",
    "\n",
    "    def __init__(self, x, y, t, u, v):\n",
    "        \n",
    "        self.x = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32, requires_grad=True)\n",
    "        self.t = torch.tensor(t, dtype=torch.float32, requires_grad=True)\n",
    "        self.u = torch.tensor(u, dtype=torch.float32, requires_grad=True)\n",
    "        self.v = torch.tensor(v, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "        self.null = torch.zeros_like(self.x)\n",
    "\n",
    "        self.neuralnet()\n",
    "\n",
    "        self.optimizer = torch.optim.LBFGS(self.net.parameters(), lr= 1, max_iter= 10000, max_eval= 5000, history_size= 50, line_search_fn= 'strong_wolfe', tolerance_change= 0.5* np.finfo(float).eps, tolerance_grad= 1e-05)\n",
    "\n",
    "        self.mse = torch.nn.MSELoss()\n",
    "        self.loss = 0 \n",
    "        self.iter = 0\n",
    "\n",
    "\n",
    "    def neuralnet(self):\n",
    "\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 20), torch.nn.Tanh(),\n",
    "            torch.nn.Linear(20, 2))\n",
    "        \n",
    "    def navier_stokes(self, x, y, t):\n",
    "        lambda_1, lambda_2 = 1., 1.\n",
    "\n",
    "        result = self.net(torch.cat([x, y, t], 1))\n",
    "        psi, p = result[:, 0:1], result[:, 1:2]\n",
    "\n",
    "        u = torch.autograd.grad(psi, y, torch.ones_like(x), create_graph=True)[0]\n",
    "        v = -torch.autograd.grad(psi, x, torch.ones_like(y), create_graph=True)[0]\n",
    "\n",
    "        p_x = torch.autograd.grad(p, x, torch.ones_like(x), create_graph=True)[0]\n",
    "        p_y = torch.autograd.grad(p, y, torch.ones_like(y), create_graph=True)[0]\n",
    "\n",
    "        u_t = torch.autograd.grad(u, t, torch.ones_like(t), create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, torch.ones_like(x), create_graph=True)[0]\n",
    "        u_y = torch.autograd.grad(u, y, torch.ones_like(y), create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, torch.ones_like(x), create_graph=True)[0]\n",
    "        u_yy = torch.autograd.grad(u_y, y, torch.ones_like(y), create_graph=True)[0]\n",
    "\n",
    "        v_t = torch.autograd.grad(v, t, torch.ones_like(t), create_graph=True)[0]\n",
    "        v_x = torch.autograd.grad(v, x, torch.ones_like(x), create_graph=True)[0]\n",
    "        v_y = torch.autograd.grad(v, y, torch.ones_like(y), create_graph=True)[0]\n",
    "        v_xx = torch.autograd.grad(v_x, x, torch.ones_like(x), create_graph=True)[0]\n",
    "        v_yy = torch.autograd.grad(v_y, y, torch.ones_like(y), create_graph=True)[0]\n",
    "\n",
    "        f_u = u_t + lambda_1 * (u*u_x + v*u_y) + p_x - lambda_2 * (u_xx + u_yy)\n",
    "        f_v = v_t + lambda_1 * (u*v_x + v*v_y) + p_y - lambda_2 * (v_xx + v_yy)\n",
    "\n",
    "        return u, v, f_u, f_v\n",
    "    \n",
    "    def closure(self):\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        u_pred, v_pred, f_u_pred, f_v_pred = self.navier_stokes(self.x, self.y, self.t)\n",
    "\n",
    "        loss_u = self.mse(u_pred, self.u)\n",
    "        loss_v = self.mse(v_pred, self.v)\n",
    "        loss_f_u = self.mse(f_u_pred, self.null)\n",
    "        loss_f_v = self.mse(f_v_pred, self.null)\n",
    "\n",
    "        self.loss = loss_u + loss_v + loss_f_u + loss_f_v\n",
    "\n",
    "        self.loss.backward()\n",
    "\n",
    "        self.iter += 1\n",
    "        if not self.iter % 10:\n",
    "            print(\"Iteration: {:} Loss: {:.6e}\".format(self.iter, self.loss))\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def train_nn(self):\n",
    "        \n",
    "        self.net.train()\n",
    "        self.optimizer.step(self.closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10 Loss: 6.637693e+14\n",
      "Iteration: 20 Loss: 1.219811e+00\n",
      "Iteration: 30 Loss: 2.572105e-01\n",
      "Iteration: 40 Loss: 1.959958e-01\n",
      "Iteration: 50 Loss: 1.732053e-01\n",
      "Iteration: 60 Loss: 1.619789e-01\n",
      "Iteration: 70 Loss: 1.558117e-01\n",
      "Iteration: 80 Loss: 1.497016e-01\n",
      "Iteration: 90 Loss: 1.462971e-01\n",
      "Iteration: 100 Loss: 1.363193e-01\n",
      "Iteration: 110 Loss: 1.337016e-01\n",
      "Iteration: 120 Loss: 1.319977e-01\n",
      "Iteration: 130 Loss: 1.307526e-01\n",
      "Iteration: 140 Loss: 1.296176e-01\n",
      "Iteration: 150 Loss: 1.288105e-01\n",
      "Iteration: 160 Loss: 1.281347e-01\n",
      "Iteration: 170 Loss: 1.277159e-01\n",
      "Iteration: 180 Loss: 1.271963e-01\n",
      "Iteration: 190 Loss: 1.266118e-01\n",
      "Iteration: 200 Loss: 1.260761e-01\n",
      "Iteration: 210 Loss: 1.259084e-01\n",
      "Iteration: 220 Loss: 1.254972e-01\n",
      "Iteration: 230 Loss: 1.252136e-01\n",
      "Iteration: 240 Loss: 1.250387e-01\n",
      "Iteration: 250 Loss: 1.248327e-01\n",
      "Iteration: 260 Loss: 1.245429e-01\n",
      "Iteration: 270 Loss: 1.243961e-01\n",
      "Iteration: 280 Loss: 1.242992e-01\n",
      "Iteration: 290 Loss: 1.241465e-01\n",
      "Iteration: 300 Loss: 1.240273e-01\n",
      "Iteration: 310 Loss: 1.239294e-01\n",
      "Iteration: 320 Loss: 1.238025e-01\n",
      "Iteration: 330 Loss: 1.236712e-01\n",
      "Iteration: 340 Loss: 1.235611e-01\n",
      "Iteration: 350 Loss: 1.233543e-01\n",
      "Iteration: 360 Loss: 1.232312e-01\n",
      "Iteration: 370 Loss: 1.231484e-01\n",
      "Iteration: 380 Loss: 1.230555e-01\n",
      "Iteration: 390 Loss: 1.229686e-01\n",
      "Iteration: 400 Loss: 1.228598e-01\n",
      "Iteration: 410 Loss: 1.227558e-01\n",
      "Iteration: 420 Loss: 1.226748e-01\n",
      "Iteration: 430 Loss: 1.226409e-01\n",
      "Iteration: 440 Loss: 1.226344e-01\n",
      "Iteration: 450 Loss: 1.225025e-01\n",
      "Iteration: 460 Loss: 1.224292e-01\n",
      "Iteration: 470 Loss: 1.223351e-01\n",
      "Iteration: 480 Loss: 1.222993e-01\n",
      "Iteration: 490 Loss: 1.221507e-01\n",
      "Iteration: 500 Loss: 1.220414e-01\n",
      "Iteration: 510 Loss: 1.218847e-01\n",
      "Iteration: 520 Loss: 1.217951e-01\n",
      "Iteration: 530 Loss: 1.217505e-01\n",
      "Iteration: 540 Loss: 1.217015e-01\n",
      "Iteration: 550 Loss: 1.216187e-01\n",
      "Iteration: 560 Loss: 1.215758e-01\n",
      "Iteration: 570 Loss: 1.214892e-01\n",
      "Iteration: 580 Loss: 1.214374e-01\n",
      "Iteration: 590 Loss: 1.213857e-01\n",
      "Iteration: 600 Loss: 1.212972e-01\n",
      "Iteration: 610 Loss: 1.211559e-01\n",
      "Iteration: 620 Loss: 1.210701e-01\n",
      "Iteration: 630 Loss: 1.212646e-01\n",
      "Iteration: 640 Loss: 1.208858e-01\n",
      "Iteration: 650 Loss: 1.208419e-01\n",
      "Iteration: 660 Loss: 1.207728e-01\n",
      "Iteration: 670 Loss: 1.206839e-01\n",
      "Iteration: 680 Loss: 1.206432e-01\n",
      "Iteration: 690 Loss: 1.205817e-01\n",
      "Iteration: 700 Loss: 1.205140e-01\n",
      "Iteration: 710 Loss: 1.204282e-01\n",
      "Iteration: 720 Loss: 1.203443e-01\n",
      "Iteration: 730 Loss: 1.202897e-01\n",
      "Iteration: 740 Loss: 1.201989e-01\n",
      "Iteration: 750 Loss: 1.200904e-01\n",
      "Iteration: 760 Loss: 1.200424e-01\n",
      "Iteration: 770 Loss: 1.199936e-01\n",
      "Iteration: 780 Loss: 1.199503e-01\n",
      "Iteration: 790 Loss: 1.199129e-01\n",
      "Iteration: 800 Loss: 1.198272e-01\n",
      "Iteration: 810 Loss: 1.197609e-01\n",
      "Iteration: 820 Loss: 1.197192e-01\n",
      "Iteration: 830 Loss: 1.196498e-01\n",
      "Iteration: 840 Loss: 1.196117e-01\n",
      "Iteration: 850 Loss: 1.195162e-01\n",
      "Iteration: 860 Loss: 1.194563e-01\n",
      "Iteration: 870 Loss: 1.193662e-01\n",
      "Iteration: 880 Loss: 1.192868e-01\n",
      "Iteration: 890 Loss: 1.192238e-01\n",
      "Iteration: 900 Loss: 1.192648e-01\n",
      "Iteration: 910 Loss: 1.190787e-01\n",
      "Iteration: 920 Loss: 1.189525e-01\n",
      "Iteration: 930 Loss: 1.188351e-01\n",
      "Iteration: 940 Loss: 1.187551e-01\n",
      "Iteration: 950 Loss: 1.186864e-01\n",
      "Iteration: 960 Loss: 1.186086e-01\n",
      "Iteration: 970 Loss: 1.185411e-01\n",
      "Iteration: 980 Loss: 1.184710e-01\n",
      "Iteration: 990 Loss: 1.184329e-01\n",
      "Iteration: 1000 Loss: 1.183757e-01\n",
      "Iteration: 1010 Loss: 1.182920e-01\n",
      "Iteration: 1020 Loss: 1.182256e-01\n",
      "Iteration: 1030 Loss: 1.181010e-01\n",
      "Iteration: 1040 Loss: 1.182764e-01\n",
      "Iteration: 1050 Loss: 1.178101e-01\n",
      "Iteration: 1060 Loss: 1.177098e-01\n",
      "Iteration: 1070 Loss: 1.175661e-01\n",
      "Iteration: 1080 Loss: 1.173794e-01\n",
      "Iteration: 1090 Loss: 1.171038e-01\n",
      "Iteration: 1100 Loss: 1.169070e-01\n",
      "Iteration: 1110 Loss: 1.167123e-01\n",
      "Iteration: 1120 Loss: 1.163921e-01\n",
      "Iteration: 1130 Loss: 1.159745e-01\n",
      "Iteration: 1140 Loss: 1.152368e-01\n",
      "Iteration: 1150 Loss: 1.148612e-01\n",
      "Iteration: 1160 Loss: 1.146308e-01\n",
      "Iteration: 1170 Loss: 1.143003e-01\n",
      "Iteration: 1180 Loss: 1.138453e-01\n",
      "Iteration: 1190 Loss: 1.135694e-01\n",
      "Iteration: 1200 Loss: 1.129592e-01\n",
      "Iteration: 1210 Loss: 1.126301e-01\n",
      "Iteration: 1220 Loss: 1.122228e-01\n",
      "Iteration: 1230 Loss: 1.119314e-01\n",
      "Iteration: 1240 Loss: 1.115327e-01\n",
      "Iteration: 1250 Loss: 1.112795e-01\n",
      "Iteration: 1260 Loss: 1.109720e-01\n",
      "Iteration: 1270 Loss: 1.107801e-01\n",
      "Iteration: 1280 Loss: 1.104651e-01\n",
      "Iteration: 1290 Loss: 1.099779e-01\n",
      "Iteration: 1300 Loss: 1.097688e-01\n",
      "Iteration: 1310 Loss: 1.096125e-01\n",
      "Iteration: 1320 Loss: 1.093415e-01\n",
      "Iteration: 1330 Loss: 1.090973e-01\n",
      "Iteration: 1340 Loss: 1.088842e-01\n",
      "Iteration: 1350 Loss: 1.087128e-01\n",
      "Iteration: 1360 Loss: 1.085416e-01\n",
      "Iteration: 1370 Loss: 1.083643e-01\n",
      "Iteration: 1380 Loss: 1.081098e-01\n",
      "Iteration: 1390 Loss: 1.078598e-01\n",
      "Iteration: 1400 Loss: 1.077394e-01\n",
      "Iteration: 1410 Loss: 1.075146e-01\n",
      "Iteration: 1420 Loss: 1.070885e-01\n",
      "Iteration: 1430 Loss: 1.068780e-01\n",
      "Iteration: 1440 Loss: 1.066343e-01\n",
      "Iteration: 1450 Loss: 1.063432e-01\n",
      "Iteration: 1460 Loss: 1.061683e-01\n",
      "Iteration: 1470 Loss: 1.059125e-01\n",
      "Iteration: 1480 Loss: 1.057471e-01\n",
      "Iteration: 1490 Loss: 1.055696e-01\n",
      "Iteration: 1500 Loss: 1.052783e-01\n",
      "Iteration: 1510 Loss: 1.049758e-01\n",
      "Iteration: 1520 Loss: 1.047616e-01\n",
      "Iteration: 1530 Loss: 1.046587e-01\n",
      "Iteration: 1540 Loss: 1.044746e-01\n",
      "Iteration: 1550 Loss: 1.042633e-01\n",
      "Iteration: 1560 Loss: 1.040499e-01\n",
      "Iteration: 1570 Loss: 1.037699e-01\n",
      "Iteration: 1580 Loss: 1.035995e-01\n",
      "Iteration: 1590 Loss: 1.034613e-01\n",
      "Iteration: 1600 Loss: 1.032082e-01\n",
      "Iteration: 1610 Loss: 1.030685e-01\n",
      "Iteration: 1620 Loss: 1.028663e-01\n",
      "Iteration: 1630 Loss: 1.027296e-01\n",
      "Iteration: 1640 Loss: 1.025959e-01\n",
      "Iteration: 1650 Loss: 1.023780e-01\n",
      "Iteration: 1660 Loss: 1.021906e-01\n",
      "Iteration: 1670 Loss: 1.020730e-01\n",
      "Iteration: 1680 Loss: 1.019775e-01\n",
      "Iteration: 1690 Loss: 1.018810e-01\n",
      "Iteration: 1700 Loss: 1.018123e-01\n",
      "Iteration: 1710 Loss: 1.016629e-01\n",
      "Iteration: 1720 Loss: 1.015151e-01\n",
      "Iteration: 1730 Loss: 1.013686e-01\n",
      "Iteration: 1740 Loss: 1.012167e-01\n",
      "Iteration: 1750 Loss: 1.010853e-01\n",
      "Iteration: 1760 Loss: 1.009992e-01\n",
      "Iteration: 1770 Loss: 1.008504e-01\n",
      "Iteration: 1780 Loss: 1.007913e-01\n",
      "Iteration: 1790 Loss: 1.007442e-01\n",
      "Iteration: 1800 Loss: 1.005866e-01\n",
      "Iteration: 1810 Loss: 1.005256e-01\n",
      "Iteration: 1820 Loss: 1.004208e-01\n",
      "Iteration: 1830 Loss: 1.003462e-01\n",
      "Iteration: 1840 Loss: 1.002930e-01\n",
      "Iteration: 1850 Loss: 1.002010e-01\n",
      "Iteration: 1860 Loss: 1.001216e-01\n",
      "Iteration: 1870 Loss: 1.000771e-01\n",
      "Iteration: 1880 Loss: 1.000406e-01\n",
      "Iteration: 1890 Loss: 9.996983e-02\n",
      "Iteration: 1900 Loss: 9.990294e-02\n",
      "Iteration: 1910 Loss: 9.983505e-02\n",
      "Iteration: 1920 Loss: 9.980270e-02\n",
      "Iteration: 1930 Loss: 9.975709e-02\n",
      "Iteration: 1940 Loss: 9.970299e-02\n",
      "Iteration: 1950 Loss: 9.963918e-02\n",
      "Iteration: 1960 Loss: 9.958911e-02\n",
      "Iteration: 1970 Loss: 9.949968e-02\n",
      "Iteration: 1980 Loss: 9.947425e-02\n",
      "Iteration: 1990 Loss: 9.939583e-02\n",
      "Iteration: 2000 Loss: 9.929319e-02\n",
      "Iteration: 2010 Loss: 9.925292e-02\n",
      "Iteration: 2020 Loss: 9.915268e-02\n",
      "Iteration: 2030 Loss: 9.911713e-02\n",
      "Iteration: 2040 Loss: 9.908390e-02\n",
      "Iteration: 2050 Loss: 9.902797e-02\n",
      "Iteration: 2060 Loss: 9.897937e-02\n",
      "Iteration: 2070 Loss: 9.895016e-02\n",
      "Iteration: 2080 Loss: 9.886351e-02\n",
      "Iteration: 2090 Loss: 9.881651e-02\n",
      "Iteration: 2100 Loss: 9.878943e-02\n",
      "Iteration: 2110 Loss: 9.871173e-02\n",
      "Iteration: 2120 Loss: 9.865277e-02\n",
      "Iteration: 2130 Loss: 9.860875e-02\n",
      "Iteration: 2140 Loss: 9.855060e-02\n",
      "Iteration: 2150 Loss: 9.850529e-02\n",
      "Iteration: 2160 Loss: 9.845219e-02\n",
      "Iteration: 2170 Loss: 9.840073e-02\n",
      "Iteration: 2180 Loss: 9.835672e-02\n",
      "Iteration: 2190 Loss: 9.832415e-02\n",
      "Iteration: 2200 Loss: 9.829687e-02\n",
      "Iteration: 2210 Loss: 9.824644e-02\n",
      "Iteration: 2220 Loss: 9.820171e-02\n",
      "Iteration: 2230 Loss: 9.814151e-02\n",
      "Iteration: 2240 Loss: 9.810793e-02\n",
      "Iteration: 2250 Loss: 9.805909e-02\n",
      "Iteration: 2260 Loss: 9.800672e-02\n",
      "Iteration: 2270 Loss: 9.795216e-02\n",
      "Iteration: 2280 Loss: 9.786672e-02\n",
      "Iteration: 2290 Loss: 9.783648e-02\n",
      "Iteration: 2300 Loss: 9.775919e-02\n",
      "Iteration: 2310 Loss: 9.770779e-02\n",
      "Iteration: 2320 Loss: 9.763730e-02\n",
      "Iteration: 2330 Loss: 9.759821e-02\n",
      "Iteration: 2340 Loss: 9.754225e-02\n",
      "Iteration: 2350 Loss: 9.750068e-02\n",
      "Iteration: 2360 Loss: 9.746417e-02\n",
      "Iteration: 2370 Loss: 9.743274e-02\n",
      "Iteration: 2380 Loss: 9.739184e-02\n",
      "Iteration: 2390 Loss: 9.734944e-02\n",
      "Iteration: 2400 Loss: 9.727601e-02\n",
      "Iteration: 2410 Loss: 9.722754e-02\n",
      "Iteration: 2420 Loss: 9.717873e-02\n",
      "Iteration: 2430 Loss: 9.714007e-02\n",
      "Iteration: 2440 Loss: 9.710175e-02\n",
      "Iteration: 2450 Loss: 9.707234e-02\n",
      "Iteration: 2460 Loss: 9.700347e-02\n",
      "Iteration: 2470 Loss: 9.696231e-02\n",
      "Iteration: 2480 Loss: 9.692243e-02\n",
      "Iteration: 2490 Loss: 9.685520e-02\n",
      "Iteration: 2500 Loss: 9.678707e-02\n",
      "Iteration: 2510 Loss: 9.674698e-02\n",
      "Iteration: 2520 Loss: 9.671476e-02\n",
      "Iteration: 2530 Loss: 9.666136e-02\n",
      "Iteration: 2540 Loss: 9.660827e-02\n",
      "Iteration: 2550 Loss: 9.652378e-02\n",
      "Iteration: 2560 Loss: 9.647328e-02\n",
      "Iteration: 2570 Loss: 9.641619e-02\n",
      "Iteration: 2580 Loss: 9.634833e-02\n",
      "Iteration: 2590 Loss: 9.631453e-02\n",
      "Iteration: 2600 Loss: 9.627784e-02\n",
      "Iteration: 2610 Loss: 9.623437e-02\n",
      "Iteration: 2620 Loss: 9.620629e-02\n",
      "Iteration: 2630 Loss: 9.616321e-02\n",
      "Iteration: 2640 Loss: 9.613077e-02\n",
      "Iteration: 2650 Loss: 9.605528e-02\n",
      "Iteration: 2660 Loss: 9.599763e-02\n",
      "Iteration: 2670 Loss: 9.593558e-02\n",
      "Iteration: 2680 Loss: 9.590062e-02\n",
      "Iteration: 2690 Loss: 9.584673e-02\n",
      "Iteration: 2700 Loss: 9.579973e-02\n",
      "Iteration: 2710 Loss: 9.574616e-02\n",
      "Iteration: 2720 Loss: 9.568027e-02\n",
      "Iteration: 2730 Loss: 9.560756e-02\n",
      "Iteration: 2740 Loss: 9.558213e-02\n",
      "Iteration: 2750 Loss: 9.548872e-02\n",
      "Iteration: 2760 Loss: 9.543270e-02\n",
      "Iteration: 2770 Loss: 9.536354e-02\n",
      "Iteration: 2780 Loss: 9.533241e-02\n",
      "Iteration: 2790 Loss: 9.530496e-02\n",
      "Iteration: 2800 Loss: 9.662521e-02\n",
      "Iteration: 2810 Loss: 9.522435e-02\n",
      "Iteration: 2820 Loss: 9.518839e-02\n",
      "Iteration: 2830 Loss: 9.516024e-02\n",
      "Iteration: 2840 Loss: 9.513965e-02\n",
      "Iteration: 2850 Loss: 9.511566e-02\n",
      "Iteration: 2860 Loss: 9.509607e-02\n",
      "Iteration: 2870 Loss: 9.506620e-02\n",
      "Iteration: 2880 Loss: 9.504664e-02\n",
      "Iteration: 2890 Loss: 9.501049e-02\n",
      "Iteration: 2900 Loss: 9.498189e-02\n",
      "Iteration: 2910 Loss: 9.494044e-02\n",
      "Iteration: 2920 Loss: 9.492936e-02\n",
      "Iteration: 2930 Loss: 9.490704e-02\n",
      "Iteration: 2940 Loss: 9.487652e-02\n",
      "Iteration: 2950 Loss: 9.485324e-02\n",
      "Iteration: 2960 Loss: 9.482549e-02\n",
      "Iteration: 2970 Loss: 9.479408e-02\n",
      "Iteration: 2980 Loss: 9.475592e-02\n",
      "Iteration: 2990 Loss: 9.473033e-02\n",
      "Iteration: 3000 Loss: 9.468582e-02\n",
      "Iteration: 3010 Loss: 9.465448e-02\n",
      "Iteration: 3020 Loss: 9.461243e-02\n",
      "Iteration: 3030 Loss: 9.457009e-02\n",
      "Iteration: 3040 Loss: 9.453708e-02\n",
      "Iteration: 3050 Loss: 9.449369e-02\n",
      "Iteration: 3060 Loss: 9.444662e-02\n",
      "Iteration: 3070 Loss: 9.440820e-02\n",
      "Iteration: 3080 Loss: 9.435952e-02\n",
      "Iteration: 3090 Loss: 9.431732e-02\n",
      "Iteration: 3100 Loss: 9.427907e-02\n",
      "Iteration: 3110 Loss: 9.422796e-02\n",
      "Iteration: 3120 Loss: 9.422229e-02\n",
      "Iteration: 3130 Loss: 9.412584e-02\n",
      "Iteration: 3140 Loss: 9.409027e-02\n",
      "Iteration: 3150 Loss: 9.407083e-02\n",
      "Iteration: 3160 Loss: 9.404014e-02\n",
      "Iteration: 3170 Loss: 9.403086e-02\n",
      "Iteration: 3180 Loss: 9.400056e-02\n",
      "Iteration: 3190 Loss: 9.397820e-02\n",
      "Iteration: 3200 Loss: 9.394396e-02\n",
      "Iteration: 3210 Loss: 9.391395e-02\n",
      "Iteration: 3220 Loss: 9.389812e-02\n",
      "Iteration: 3230 Loss: 9.386876e-02\n",
      "Iteration: 3240 Loss: 9.387937e-02\n",
      "Iteration: 3250 Loss: 9.378318e-02\n",
      "Iteration: 3260 Loss: 9.375571e-02\n",
      "Iteration: 3270 Loss: 9.373822e-02\n",
      "Iteration: 3280 Loss: 9.370400e-02\n",
      "Iteration: 3290 Loss: 9.367096e-02\n",
      "Iteration: 3300 Loss: 9.365454e-02\n",
      "Iteration: 3310 Loss: 9.363961e-02\n",
      "Iteration: 3320 Loss: 9.361511e-02\n",
      "Iteration: 3330 Loss: 9.359866e-02\n",
      "Iteration: 3340 Loss: 9.356922e-02\n",
      "Iteration: 3350 Loss: 9.354732e-02\n",
      "Iteration: 3360 Loss: 9.351868e-02\n",
      "Iteration: 3370 Loss: 9.347174e-02\n",
      "Iteration: 3380 Loss: 9.397733e-02\n",
      "Iteration: 3390 Loss: 9.341794e-02\n",
      "Iteration: 3400 Loss: 9.337688e-02\n",
      "Iteration: 3410 Loss: 9.336006e-02\n",
      "Iteration: 3420 Loss: 9.333852e-02\n",
      "Iteration: 3430 Loss: 9.331091e-02\n",
      "Iteration: 3440 Loss: 9.327947e-02\n",
      "Iteration: 3450 Loss: 9.328058e-02\n",
      "Iteration: 3460 Loss: 9.325108e-02\n",
      "Iteration: 3470 Loss: 9.322163e-02\n",
      "Iteration: 3480 Loss: 9.319774e-02\n",
      "Iteration: 3490 Loss: 9.318160e-02\n",
      "Iteration: 3500 Loss: 9.315389e-02\n",
      "Iteration: 3510 Loss: 9.311676e-02\n",
      "Iteration: 3520 Loss: 9.310387e-02\n",
      "Iteration: 3530 Loss: 9.309378e-02\n",
      "Iteration: 3540 Loss: 9.306995e-02\n",
      "Iteration: 3550 Loss: 9.304796e-02\n",
      "Iteration: 3560 Loss: 9.304240e-02\n",
      "Iteration: 3570 Loss: 9.297947e-02\n",
      "Iteration: 3580 Loss: 9.296422e-02\n",
      "Iteration: 3590 Loss: 9.294867e-02\n",
      "Iteration: 3600 Loss: 9.293655e-02\n",
      "Iteration: 3610 Loss: 9.292115e-02\n",
      "Iteration: 3620 Loss: 9.288213e-02\n",
      "Iteration: 3630 Loss: 9.296094e-02\n",
      "Iteration: 3640 Loss: 9.281957e-02\n",
      "Iteration: 3650 Loss: 9.279235e-02\n",
      "Iteration: 3660 Loss: 9.277758e-02\n",
      "Iteration: 3670 Loss: 9.275603e-02\n",
      "Iteration: 3680 Loss: 9.272037e-02\n",
      "Iteration: 3690 Loss: 9.269742e-02\n",
      "Iteration: 3700 Loss: 9.266789e-02\n",
      "Iteration: 3710 Loss: 9.263470e-02\n",
      "Iteration: 3720 Loss: 9.257346e-02\n",
      "Iteration: 3730 Loss: 9.254006e-02\n",
      "Iteration: 3740 Loss: 9.250587e-02\n",
      "Iteration: 3750 Loss: 9.247653e-02\n",
      "Iteration: 3760 Loss: 9.245475e-02\n",
      "Iteration: 3770 Loss: 9.241466e-02\n",
      "Iteration: 3780 Loss: 9.238419e-02\n",
      "Iteration: 3790 Loss: 9.236063e-02\n",
      "Iteration: 3800 Loss: 9.233906e-02\n",
      "Iteration: 3810 Loss: 9.231278e-02\n",
      "Iteration: 3820 Loss: 9.227946e-02\n",
      "Iteration: 3830 Loss: 9.223602e-02\n",
      "Iteration: 3840 Loss: 9.221407e-02\n",
      "Iteration: 3850 Loss: 9.217517e-02\n",
      "Iteration: 3860 Loss: 9.216351e-02\n",
      "Iteration: 3870 Loss: 9.214504e-02\n",
      "Iteration: 3880 Loss: 9.211479e-02\n",
      "Iteration: 3890 Loss: 9.208807e-02\n",
      "Iteration: 3900 Loss: 9.206615e-02\n",
      "Iteration: 3910 Loss: 9.203321e-02\n",
      "Iteration: 3920 Loss: 9.202672e-02\n",
      "Iteration: 3930 Loss: 9.198999e-02\n",
      "Iteration: 3940 Loss: 9.198308e-02\n",
      "Iteration: 3950 Loss: 9.193217e-02\n",
      "Iteration: 3960 Loss: 9.191815e-02\n",
      "Iteration: 3970 Loss: 9.189183e-02\n",
      "Iteration: 3980 Loss: 9.187075e-02\n",
      "Iteration: 3990 Loss: 9.184157e-02\n",
      "Iteration: 4000 Loss: 9.180339e-02\n",
      "Iteration: 4010 Loss: 9.179098e-02\n",
      "Iteration: 4020 Loss: 9.176894e-02\n",
      "Iteration: 4030 Loss: 9.175123e-02\n",
      "Iteration: 4040 Loss: 9.170788e-02\n",
      "Iteration: 4050 Loss: 9.169175e-02\n",
      "Iteration: 4060 Loss: 9.168126e-02\n",
      "Iteration: 4070 Loss: 9.167135e-02\n",
      "Iteration: 4080 Loss: 9.164333e-02\n",
      "Iteration: 4090 Loss: 9.161226e-02\n",
      "Iteration: 4100 Loss: 9.157588e-02\n",
      "Iteration: 4110 Loss: 9.155831e-02\n",
      "Iteration: 4120 Loss: 9.152143e-02\n",
      "Iteration: 4130 Loss: 9.150237e-02\n",
      "Iteration: 4140 Loss: 9.147210e-02\n",
      "Iteration: 4150 Loss: 9.144538e-02\n",
      "Iteration: 4160 Loss: 9.141207e-02\n",
      "Iteration: 4170 Loss: 9.138714e-02\n",
      "Iteration: 4180 Loss: 9.137418e-02\n",
      "Iteration: 4190 Loss: 9.134413e-02\n",
      "Iteration: 4200 Loss: 9.132791e-02\n",
      "Iteration: 4210 Loss: 9.131192e-02\n",
      "Iteration: 4220 Loss: 9.128736e-02\n",
      "Iteration: 4230 Loss: 9.127701e-02\n",
      "Iteration: 4240 Loss: 9.125268e-02\n",
      "Iteration: 4250 Loss: 9.124135e-02\n",
      "Iteration: 4260 Loss: 9.121536e-02\n",
      "Iteration: 4270 Loss: 9.120122e-02\n",
      "Iteration: 4280 Loss: 9.118575e-02\n",
      "Iteration: 4290 Loss: 9.117162e-02\n",
      "Iteration: 4300 Loss: 9.115671e-02\n",
      "Iteration: 4310 Loss: 9.132180e-02\n",
      "Iteration: 4320 Loss: 9.112904e-02\n",
      "Iteration: 4330 Loss: 9.110693e-02\n",
      "Iteration: 4340 Loss: 9.122172e-02\n",
      "Iteration: 4350 Loss: 9.108310e-02\n",
      "Iteration: 4360 Loss: 9.127765e-02\n",
      "Iteration: 4370 Loss: 9.105222e-02\n",
      "Iteration: 4380 Loss: 9.102501e-02\n",
      "Iteration: 4390 Loss: 9.100679e-02\n",
      "Iteration: 4400 Loss: 9.514127e-02\n",
      "Iteration: 4410 Loss: 9.097071e-02\n",
      "Iteration: 4420 Loss: 9.095217e-02\n",
      "Iteration: 4430 Loss: 9.094307e-02\n",
      "Iteration: 4440 Loss: 9.092766e-02\n",
      "Iteration: 4450 Loss: 9.092048e-02\n",
      "Iteration: 4460 Loss: 9.091060e-02\n",
      "Iteration: 4470 Loss: 9.088658e-02\n",
      "Iteration: 4480 Loss: 9.086734e-02\n",
      "Iteration: 4490 Loss: 9.085732e-02\n",
      "Iteration: 4500 Loss: 9.084596e-02\n",
      "Iteration: 4510 Loss: 9.083380e-02\n",
      "Iteration: 4520 Loss: 9.081977e-02\n",
      "Iteration: 4530 Loss: 9.081170e-02\n",
      "Iteration: 4540 Loss: 9.079553e-02\n",
      "Iteration: 4550 Loss: 9.078308e-02\n",
      "Iteration: 4560 Loss: 9.076770e-02\n",
      "Iteration: 4570 Loss: 9.075185e-02\n",
      "Iteration: 4580 Loss: 9.074385e-02\n",
      "Iteration: 4590 Loss: 9.073061e-02\n",
      "Iteration: 4600 Loss: 9.071671e-02\n",
      "Iteration: 4610 Loss: 9.069977e-02\n",
      "Iteration: 4620 Loss: 9.075862e-02\n",
      "Iteration: 4630 Loss: 9.067351e-02\n",
      "Iteration: 4640 Loss: 9.066610e-02\n",
      "Iteration: 4650 Loss: 9.065565e-02\n",
      "Iteration: 4660 Loss: 9.064367e-02\n",
      "Iteration: 4670 Loss: 9.063783e-02\n",
      "Iteration: 4680 Loss: 9.062918e-02\n",
      "Iteration: 4690 Loss: 9.061382e-02\n",
      "Iteration: 4700 Loss: 9.058831e-02\n",
      "Iteration: 4710 Loss: 9.057304e-02\n",
      "Iteration: 4720 Loss: 9.254119e-02\n",
      "Iteration: 4730 Loss: 9.053455e-02\n",
      "Iteration: 4740 Loss: 9.051014e-02\n",
      "Iteration: 4750 Loss: 9.050286e-02\n",
      "Iteration: 4760 Loss: 9.048409e-02\n",
      "Iteration: 4770 Loss: 9.047226e-02\n",
      "Iteration: 4780 Loss: 9.045019e-02\n",
      "Iteration: 4790 Loss: 9.042893e-02\n",
      "Iteration: 4800 Loss: 9.041805e-02\n",
      "Iteration: 4810 Loss: 9.039775e-02\n",
      "Iteration: 4820 Loss: 9.037851e-02\n",
      "Iteration: 4830 Loss: 9.035960e-02\n",
      "Iteration: 4840 Loss: 9.034799e-02\n",
      "Iteration: 4850 Loss: 9.032814e-02\n",
      "Iteration: 4860 Loss: 9.031604e-02\n",
      "Iteration: 4870 Loss: 9.030812e-02\n",
      "Iteration: 4880 Loss: 9.029613e-02\n",
      "Iteration: 4890 Loss: 9.028555e-02\n",
      "Iteration: 4900 Loss: 9.027747e-02\n",
      "Iteration: 4910 Loss: 9.026417e-02\n",
      "Iteration: 4920 Loss: 9.024620e-02\n",
      "Iteration: 4930 Loss: 9.061891e-02\n",
      "Iteration: 4940 Loss: 9.021090e-02\n",
      "Iteration: 4950 Loss: 9.019422e-02\n",
      "Iteration: 4960 Loss: 9.017614e-02\n",
      "Iteration: 4970 Loss: 9.015805e-02\n",
      "Iteration: 4980 Loss: 9.014527e-02\n",
      "Iteration: 4990 Loss: 9.013547e-02\n",
      "Iteration: 5000 Loss: 9.011362e-02\n"
     ]
    }
   ],
   "source": [
    "N_train = 5000\n",
    "\n",
    "pinn_navier_stokes = PINN(x_train, y_train, t_train, u_train, v_train)\n",
    "pinn_navier_stokes.train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(pinn_navier_stokes.net, 'pinn_navier_stokes.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
