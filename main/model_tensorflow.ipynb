{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define random seed\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sio.loadmat('cylinder_nektar_wake.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_star = data['U_star'] # N x 2 x T\n",
    "p_star = data['p_star'] # N x 2 x T\n",
    "t_star = data ['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2 \n",
    "\n",
    "N = X_star.shape[0] # 5000\n",
    "T = t_star.shape[0] # 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select test data for the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = X_star[:, 0:1]\n",
    "y_test = X_star[:, 1:2]\n",
    "p_test = p_star[:, 0:1]\n",
    "u_test = U_star[:, 0:1, 0]\n",
    "t_test = np.ones((x_test.shape[0], x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.tile(X_star[:, 0:1], (1, T))  # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T))  # N x T\n",
    "TT = np.tile(t_star, (1, N)).T  # N x T\n",
    "\n",
    "UU = U_star[:, 0, :]  # N x T\n",
    "VV = U_star[:, 1, :]  # N x T\n",
    "PP = p_star  # N x T\n",
    "\n",
    "x = XX.flatten()[:, None]  # NT x 1\n",
    "y = YY.flatten()[:, None]  # NT x 1\n",
    "t = TT.flatten()[:, None]  # NT x 1\n",
    "\n",
    "u = UU.flatten()[:, None]  # NT x 1\n",
    "v = VV.flatten()[:, None]  # NT x 1\n",
    "p = PP.flatten()[:, None]  # NT x 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select training data for the PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noiseless data\n",
    "N_train = 5000\n",
    "idx = np.random.choice(N*T, N_train, replace=False)\n",
    "x_train = x[idx,:] # 5000 x 1\n",
    "y_train = y[idx,:]\n",
    "t_train = t[idx,:]\n",
    "u_train = u[idx,:]\n",
    "v_train = v[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy data\n",
    "noise = 0.01        \n",
    "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "v_train = v_train + noise*np.std(v_train)*np.random.randn(v_train.shape[0], v_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test to implement it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">420</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m12\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │            \u001b[38;5;34m80\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_32 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_33 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_34 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_35 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_36 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_37 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_38 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │           \u001b[38;5;34m420\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_39 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m42\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,074</span> (12.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,074\u001b[0m (12.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,074</span> (12.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,074\u001b[0m (12.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(3,)),\n",
    "    tf.keras.layers.Dense(3, activation='tanh'),\n",
    "    *[tf.keras.layers.Dense(20, activation='tanh') for _ in range(8)],\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train() Funktion programmieren, die zuerst mit Adam Optimizer trainiert und dann mit L-BFGS-B Optimizer weiter trainiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "t_train = tf.convert_to_tensor(t_train, dtype=tf.float32)\n",
    "u_train = tf.convert_to_tensor(u_train, dtype=tf.float32)\n",
    "v_train = tf.convert_to_tensor(v_train, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.optimizers.adam.Adam at 0x33a5ab500>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(tf.concat([x_train, y_train, t_train], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5000, 3), dtype=float32, numpy=\n",
       "array([[ 7.5050507 , -0.12244898, 14.7       ],\n",
       "       [ 1.4949495 , -1.3469387 ,  9.6       ],\n",
       "       [ 1.5656565 , -1.0204082 , 10.2       ],\n",
       "       ...,\n",
       "       [ 5.4545455 , -0.53061223,  7.5       ],\n",
       "       [ 1.5656565 , -0.20408164,  1.        ],\n",
       "       [ 6.79798   , -0.93877554, 14.1       ]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[[ 7.5050507  -0.12244898 14.7       ]\n [ 1.4949495  -1.3469387   9.6       ]\n [ 1.5656565  -1.0204082  10.2       ]\n ...\n [ 5.4545455  -0.53061223  7.5       ]\n [ 1.5656565  -0.20408164  1.        ]\n [ 6.79798    -0.93877554 14.1       ]] (of type <class 'numpy.ndarray'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/trainers/data_adapters/__init__.py:113\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized data type: x=[[ 7.5050507  -0.12244898 14.7       ]\n [ 1.4949495  -1.3469387   9.6       ]\n [ 1.5656565  -1.0204082  10.2       ]\n ...\n [ 5.4545455  -0.53061223  7.5       ]\n [ 1.5656565  -0.20408164  1.        ]\n [ 6.79798    -0.93877554 14.1       ]] (of type <class 'numpy.ndarray'>)"
     ]
    }
   ],
   "source": [
    "model.fit(X, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# GradientTape returned None weil ich mein Model noch nicht trainiert habe, ergo sind meine Gewichte noch nicht initialisiert und ich kann deswegen auch keinen Gradienten berechnen\n",
    "\n",
    "lambda_1 = tf.constant([0.01], dtype= tf.float32)\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "t_train = tf.convert_to_tensor(t_train, dtype=tf.float32)\n",
    "\n",
    "results = model(tf.concat([x_train, y_train, t_train], axis=1))\n",
    "\n",
    "psi, p = results[:, 0:1], results[:, 1:2]\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    tape.watch([psi, p, x_train, y_train, t_train])\n",
    "    u = tape.gradient(psi, y_train)\n",
    "    v = tape.gradient(psi, x_train)\n",
    "    p_x = tape.gradient(p, x_train)\n",
    "    p_y = tape.gradient(p, y_train)\n",
    "    \n",
    "    u_t = tape.gradient(u, t_train)\n",
    "    u_x = tape.gradient(u, x_train)\n",
    "    u_y = tape.gradient(u, y_train)\n",
    "    u_xx = tape.gradient(u_x, x_train)\n",
    "    u_yy = tape.gradient(u_y, y_train)\n",
    "\n",
    "    v_t = tape.gradient(-v, t_train)\n",
    "    v_x = tape.gradient(-v, x_train)\n",
    "    v_y = tape.gradient(-v, y_train)\n",
    "    v_xx = tape.gradient(v_x, x_train)\n",
    "    v_yy = tape.gradient(v_y, y_train)\n",
    "\n",
    "f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_1*(u_xx + u_yy)\n",
    "f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_1*(v_xx + v_yy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Callable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(loss: Union[callable, str]):\n",
    "    '''funciton creates a neural network model with desired loss function'''\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(3, activation=\"tanh\", input_shape=(3,)),\n",
    "        *[tf.keras.layers.Dense(20, activation='tanh') for _ in range(8)],\n",
    "        tf.keras.layers.Dense(2)\n",
    "    ])   \n",
    "\n",
    "    model.compile(loss= loss, optimizer= \"adam\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(model, x, y, t,):\n",
    "    '''Use the NN output of the stream and pressure fields to invoke the Navier Stokes equations'''\n",
    "\n",
    "    results = model(x, y, t)\n",
    "    \n",
    "    psi, p = results[:, 0:1], results[:, 1:2]\n",
    "        \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([psi, p, x, y, t])\n",
    "        u = tape.gradient(psi, y)\n",
    "        v = tape.gradient(psi, x)\n",
    "        p_x = tape.gradient(p, x)\n",
    "        p_y = tape.gradient(p, y)\n",
    "        \n",
    "        u_t = tape.gradient(u, t)\n",
    "        u_x = tape.gradient(u, x)\n",
    "        u_y = tape.gradient(u, y)\n",
    "        u_xx = tape.gradient(u_x, x)\n",
    "        u_yy = tape.gradient(u_y, y)\n",
    "\n",
    "        v_t = tape.gradient(-v, t)\n",
    "        v_x = tape.gradient(-v, x)\n",
    "        v_y = tape.gradient(-v, y)\n",
    "        v_xx = tape.gradient(v_x, x)\n",
    "        v_yy = tape.gradient(v_y, y)\n",
    "\n",
    "    f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_1*(u_xx + u_yy)\n",
    "    f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_1*(v_xx + v_yy)\n",
    "\n",
    "    return u, v, p, f_u, f_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "\n",
    "    def __init__(self, x, y, t, u, v,):\n",
    "\n",
    "        self.x = tf.Variable(x, dtype=tf.float32)\n",
    "        self.y = tf.Variable(y, dtype=tf.float32)\n",
    "        self.t = tf.Variable(t, dtype=tf.float32)\n",
    "        self.u = tf.Variable(u, dtype=tf.float32)\n",
    "        self.v = tf.Variable(v, dtype=tf.float32)\n",
    "        self.null = tf.zeros_like(self.x)\n",
    "\n",
    "        self.network()\n",
    "    \n",
    "        self.mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def network(self, loss: Union[callable, str]):\n",
    "\n",
    "        self.model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Input(shape=(3,)),\n",
    "            *[tf.keras.layers.Dense(20, activation='tanh') for _ in range(8)],\n",
    "            tf.keras.layers.Dense(2)\n",
    "        ])   \n",
    "\n",
    "        self.model.compile(loss= loss, optimizer= \"adam\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def function(self, x, y, t):\n",
    "        \n",
    "        x = tf.Variable(x_train, dtype=tf.float32)\n",
    "        y = tf.Variable(y_train, dtype=tf.float32)\n",
    "        t = tf.Variable(t_train, dtype=tf.float32)\n",
    "        \n",
    "        lambda_1 = tf.constant([0.01], dtype= tf.float32)\n",
    "\n",
    "        results = self.net(tf.concat([x, y, t], axis=1))\n",
    "        psi, p = results[:, 0:1], results[:, 1:2]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch([psi, p])\n",
    "            u = tape.gradient(psi, y)\n",
    "            v = tape.gradient(psi, x)\n",
    "            p_x = tape.gradient(p, x)\n",
    "            p_y = tape.gradient(p, y)\n",
    "            \n",
    "            u_t = tape.gradient(u, t)\n",
    "            u_x = tape.gradient(u, x)\n",
    "            u_y = tape.gradient(u, y)\n",
    "            u_xx = tape.gradient(u_x, x)\n",
    "            u_yy = tape.gradient(u_y, y)\n",
    "\n",
    "            v_t = tape.gradient(-v, t)\n",
    "            v_x = tape.gradient(-v, x)\n",
    "            v_y = tape.gradient(-v, y)\n",
    "            v_xx = tape.gradient(v_x, x)\n",
    "            v_yy = tape.gradient(v_y, y)\n",
    "\n",
    "        f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_1*(u_xx + u_yy)\n",
    "        f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_1*(v_xx + v_yy)\n",
    "\n",
    "        return u, v, p, f_u, f_v\n",
    "    \n",
    "    def train(self, epochs):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                u_pred, v_pred, p_pred, f_u_pred, f_v_pred = self.function(self.x, self.y, self.t)\n",
    "\n",
    "                loss = self.mse(self.u, u_pred) + self.mse(self.v, v_pred) + self.mse(self.null, f_u_pred) + self.mse(self.null, f_v_pred)\n",
    "\n",
    "            grads = tape.gradient(loss, self.net.trainable_variables)\n",
    "\n",
    "            self.optimizer.apply_gradients(zip(grads, self.net.trainable_variables))\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss.numpy()}')\n",
    "\n",
    "    def plot_loss(history):\n",
    "\n",
    "        fig, ax = t.subplots()\n",
    "        ax.plot(history.history['loss'], label='loss')\n",
    "        ax.plot(history.history['val_loss'], label='val_loss')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PINN(x_train, y_train, t_train, u_train, v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[107], line 65\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 65\u001b[0m         u_pred, v_pred, p_pred, f_u_pred, f_v_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu, u_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv, v_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnull, f_u_pred) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnull, f_v_pred)\n\u001b[1;32m     69\u001b[0m     grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "Cell \u001b[0;32mIn[107], line 42\u001b[0m, in \u001b[0;36mPINN.function\u001b[0;34m(self, x, y, t)\u001b[0m\n\u001b[1;32m     39\u001b[0m p_x \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(p, x)\n\u001b[1;32m     40\u001b[0m p_y \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(p, y)\n\u001b[0;32m---> 42\u001b[0m u_t \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m u_x \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(u, x)\n\u001b[1;32m     44\u001b[0m u_y \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(u, y)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:1023\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[1;32m   1012\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtape inside its context is significantly less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient in order to compute higher order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1023\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `target` should be a list or nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1025\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiated, but received None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1027\u001b[0m flat_targets \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1028\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(target))\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m# TODO(b/246997907): Remove this once\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# ResourceVariableGradient.get_gradient_components returns the handle.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "model.train(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN():\n",
    "\n",
    "    def __init__(self, x, y, t, u, v):\n",
    "\n",
    "        self.x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "        self.y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "        self.t = tf.convert_to_tensor(t, dtype=tf.float32)\n",
    "        self.u = tf.convert_to_tensor(u, dtype=tf.float32)\n",
    "        self.v = tf.convert_to_tensor(v, dtype=tf.float32)\n",
    "\n",
    "        # Initialize NN\n",
    "        self.initialize_nn()\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.lambda_1 = tf.Variable([0.0], dtype= tf.float32)\n",
    "        self.lambda_2 = tf.Variable([0.0], dtype= tf.float32) \n",
    "\n",
    "\n",
    "    def initialize_nn(self):\n",
    "        self.network = tf.keras.Sequential([\n",
    "            tf.keras.Input(shape=(3,)),\n",
    "            *[tf.keras.layers.Dense(20, activation='tanh') for _ in range(8)],\n",
    "            tf.keras.layers.Dense(2, activation='tanh')\n",
    "        ])\n",
    "\n",
    "    def NavierStokes(self, x, y, t):\n",
    "\n",
    "        lambda_1 = self.lambda_1\n",
    "        lambda_2 = self.lambda_2\n",
    "\n",
    "        psi_and_p = self.network(tf.concat([x, y, t], axis=1))\n",
    "        psi = psi_and_p[:, 0:1]\n",
    "        p = psi_and_p[:, 1:2]\n",
    "\n",
    "        u = tf.gradients(psi, y)[0]\n",
    "        v = -tf.gradients(psi, x)[0]\n",
    "\n",
    "        u_t = tf.gradients(u, t)[0]\n",
    "        u_x = tf.gradients(u, x)[0]\n",
    "        u_y = tf.gradients(u, y)[0]\n",
    "        u_xx = tf.gradients(u_x, x)[0]\n",
    "        u_yy = tf.gradients(u_y, y)[0]\n",
    "\n",
    "        v_t = tf.gradients(v, t)[0]\n",
    "        v_x = tf.gradients(v, x)[0]\n",
    "        v_y = tf.gradients(v, y)[0]\n",
    "        v_xx = tf.gradients(v_x, x)[0]\n",
    "        v_yy = tf.gradients(v_y, y)[0]\n",
    "\n",
    "        p_x = tf.gradients(p, x)[0]\n",
    "        p_y = tf.gradients(p, y)[0]\n",
    "\n",
    "        f_u = u_t + lambda_1*(u*u_x + v*u_y) + p_x - lambda_2*(u_xx + u_yy)\n",
    "        f_v = v_t + lambda_1*(u*v_x + v*v_y) + p_y - lambda_2*(v_xx + v_yy)\n",
    "\n",
    "        return u, v, p, f_u, f_v\n",
    "    \n",
    "    def loss_function(self):\n",
    "\n",
    "        u_pred, v_pred, p_pred, f_u_pred, f_v_pred = self.net_NS(self.x, self.y, self.t)\n",
    "\n",
    "        # Calculate loss of predicted velocity values\n",
    "        u_loss = tf.reduce_sum(tf.square(self.u - u_pred))\n",
    "        v_loss = tf.reduce_sum(tf.square(self.v - v_pred))\n",
    "\n",
    "        # Calculate residuals of Navier-Stokes equations\n",
    "        f_u_loss = tf.reduce_sum(tf.square(f_u_pred))\n",
    "        f_v_loss = tf.reduce_sum(tf.square(f_v_pred))\n",
    "\n",
    "        # Combine losses\n",
    "        loss = u_loss + v_loss + f_u_loss + f_v_loss\n",
    "        return loss\n",
    "\n",
    "    def train(self, epochs):\n",
    "\n",
    "        self.network.compile(optimizer='adam', loss=self.loss_function)\n",
    "        self.network.fit([self.x, self.y, self.t], [self.u, self.v], epochs=epochs)\n",
    "\n",
    "        # After training with adam optimzer, use L-BFGS-B optimizer to minimize the loss function \n",
    "        # -> PINN dont work solely with Adam due to Adam optimizer finding local minima\n",
    "        results = tfp.optimizer.lbfgs_minimize(\n",
    "            self.loss_function,\n",
    "            initial_position=self.network.trainable_variables, max_iterations=1000\n",
    "            )\n",
    "        \n",
    "        # Update model parameters after L-BFGS-B optimization\n",
    "        for var, new_var in zip(self.network.trainable_variables, results.position):\n",
    "            var.assign(new_var)\n",
    "\n",
    "    def predict(self, x_star, y_star, t_star):\n",
    "\n",
    "        x_star = tf.convert_to_tensor(x_star, dtype=tf.float32)\n",
    "        y_star = tf.convert_to_tensor(y_star, dtype=tf.float32)\n",
    "        t_star = tf.convert_to_tensor(t_star, dtype=tf.float32)\n",
    "\n",
    "        u_star, v_star, p_star, _, _ = self.net_NS(x_star, y_star, t_star)\n",
    "\n",
    "        return u_star.numpy(), v_star.numpy(), p_star.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_90\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(None, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)')\n  • training=True\n  • mask=('None', 'None', 'None')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m PINN(x_train, y_train, t_train, u_train, v_train)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[90], line 77\u001b[0m, in \u001b[0;36mPINN.train\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# After training with adam optimzer, use L-BFGS-B optimizer to minimize the loss function \u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# -> PINN dont work solely with Adam due to Adam optimizer finding local minima\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     results \u001b[38;5;241m=\u001b[39m tfp\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlbfgs_minimize(\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function,\n\u001b[1;32m     83\u001b[0m         initial_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork\u001b[38;5;241m.\u001b[39mtrainable_variables, max_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     84\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[1;32m    224\u001b[0m             value,\n\u001b[1;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    226\u001b[0m         }:\n\u001b[0;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m             )\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense_90\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (None, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=('tf.Tensor(shape=(None, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)', 'tf.Tensor(shape=(None, 1), dtype=float32)')\n  • training=True\n  • mask=('None', 'None', 'None')"
     ]
    }
   ],
   "source": [
    "model = PINN(x_train, y_train, t_train, u_train, v_train)\n",
    "model.train(epochs=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
